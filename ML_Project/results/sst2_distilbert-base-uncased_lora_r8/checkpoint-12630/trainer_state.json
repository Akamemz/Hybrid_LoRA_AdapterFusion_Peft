{
  "best_global_step": 4210,
  "best_metric": 0.28028979897499084,
  "best_model_checkpoint": "/Users/timurabdygulov/Documents/GitHub/Hybrid_LoRA-Data-Centric_Improvements/ML_Project/results/sst2_distilbert-base-uncased_lora_r8/checkpoint-4210",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 12630,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.023752969121140142,
      "grad_norm": 1.274478554725647,
      "learning_rate": 4.9608076009501194e-05,
      "loss": 0.6292,
      "step": 100
    },
    {
      "epoch": 0.047505938242280284,
      "grad_norm": 1.9673714637756348,
      "learning_rate": 4.921219319081552e-05,
      "loss": 0.3756,
      "step": 200
    },
    {
      "epoch": 0.07125890736342043,
      "grad_norm": 2.4013283252716064,
      "learning_rate": 4.8816310372129854e-05,
      "loss": 0.3233,
      "step": 300
    },
    {
      "epoch": 0.09501187648456057,
      "grad_norm": 1.5720536708831787,
      "learning_rate": 4.842042755344418e-05,
      "loss": 0.3668,
      "step": 400
    },
    {
      "epoch": 0.1187648456057007,
      "grad_norm": 2.710692882537842,
      "learning_rate": 4.8024544734758514e-05,
      "loss": 0.3386,
      "step": 500
    },
    {
      "epoch": 0.14251781472684086,
      "grad_norm": 1.8209140300750732,
      "learning_rate": 4.762866191607284e-05,
      "loss": 0.3636,
      "step": 600
    },
    {
      "epoch": 0.166270783847981,
      "grad_norm": 2.7254226207733154,
      "learning_rate": 4.7232779097387175e-05,
      "loss": 0.3191,
      "step": 700
    },
    {
      "epoch": 0.19002375296912113,
      "grad_norm": 2.1523780822753906,
      "learning_rate": 4.683689627870151e-05,
      "loss": 0.3321,
      "step": 800
    },
    {
      "epoch": 0.21377672209026127,
      "grad_norm": 3.7675588130950928,
      "learning_rate": 4.6441013460015835e-05,
      "loss": 0.3575,
      "step": 900
    },
    {
      "epoch": 0.2375296912114014,
      "grad_norm": 4.1181864738464355,
      "learning_rate": 4.604513064133017e-05,
      "loss": 0.3112,
      "step": 1000
    },
    {
      "epoch": 0.26128266033254155,
      "grad_norm": 2.154846668243408,
      "learning_rate": 4.5649247822644495e-05,
      "loss": 0.3141,
      "step": 1100
    },
    {
      "epoch": 0.2850356294536817,
      "grad_norm": 2.5980968475341797,
      "learning_rate": 4.525336500395883e-05,
      "loss": 0.2721,
      "step": 1200
    },
    {
      "epoch": 0.3087885985748218,
      "grad_norm": 2.5558831691741943,
      "learning_rate": 4.4857482185273156e-05,
      "loss": 0.3166,
      "step": 1300
    },
    {
      "epoch": 0.332541567695962,
      "grad_norm": 4.84277868270874,
      "learning_rate": 4.446159936658749e-05,
      "loss": 0.2973,
      "step": 1400
    },
    {
      "epoch": 0.35629453681710216,
      "grad_norm": 4.379382133483887,
      "learning_rate": 4.406571654790182e-05,
      "loss": 0.3031,
      "step": 1500
    },
    {
      "epoch": 0.38004750593824227,
      "grad_norm": 1.0758798122406006,
      "learning_rate": 4.366983372921615e-05,
      "loss": 0.2878,
      "step": 1600
    },
    {
      "epoch": 0.40380047505938244,
      "grad_norm": 2.3256688117980957,
      "learning_rate": 4.3273950910530483e-05,
      "loss": 0.3144,
      "step": 1700
    },
    {
      "epoch": 0.42755344418052255,
      "grad_norm": 2.181724786758423,
      "learning_rate": 4.287806809184482e-05,
      "loss": 0.302,
      "step": 1800
    },
    {
      "epoch": 0.4513064133016627,
      "grad_norm": 1.0175093412399292,
      "learning_rate": 4.248218527315915e-05,
      "loss": 0.3063,
      "step": 1900
    },
    {
      "epoch": 0.4750593824228028,
      "grad_norm": 2.670928478240967,
      "learning_rate": 4.208630245447348e-05,
      "loss": 0.305,
      "step": 2000
    },
    {
      "epoch": 0.498812351543943,
      "grad_norm": 2.288891553878784,
      "learning_rate": 4.169041963578781e-05,
      "loss": 0.3169,
      "step": 2100
    },
    {
      "epoch": 0.5225653206650831,
      "grad_norm": 2.0391316413879395,
      "learning_rate": 4.1294536817102145e-05,
      "loss": 0.2936,
      "step": 2200
    },
    {
      "epoch": 0.5463182897862233,
      "grad_norm": 2.5684101581573486,
      "learning_rate": 4.089865399841647e-05,
      "loss": 0.2561,
      "step": 2300
    },
    {
      "epoch": 0.5700712589073634,
      "grad_norm": 3.533267021179199,
      "learning_rate": 4.0502771179730805e-05,
      "loss": 0.2691,
      "step": 2400
    },
    {
      "epoch": 0.5938242280285035,
      "grad_norm": 2.245142936706543,
      "learning_rate": 4.010688836104513e-05,
      "loss": 0.285,
      "step": 2500
    },
    {
      "epoch": 0.6175771971496437,
      "grad_norm": 2.230543851852417,
      "learning_rate": 3.9711005542359465e-05,
      "loss": 0.3082,
      "step": 2600
    },
    {
      "epoch": 0.6413301662707839,
      "grad_norm": 2.6138834953308105,
      "learning_rate": 3.931512272367379e-05,
      "loss": 0.291,
      "step": 2700
    },
    {
      "epoch": 0.665083135391924,
      "grad_norm": 2.327930212020874,
      "learning_rate": 3.8919239904988126e-05,
      "loss": 0.3044,
      "step": 2800
    },
    {
      "epoch": 0.6888361045130641,
      "grad_norm": 2.4143102169036865,
      "learning_rate": 3.852335708630246e-05,
      "loss": 0.2968,
      "step": 2900
    },
    {
      "epoch": 0.7125890736342043,
      "grad_norm": 1.4532694816589355,
      "learning_rate": 3.8127474267616786e-05,
      "loss": 0.2895,
      "step": 3000
    },
    {
      "epoch": 0.7363420427553444,
      "grad_norm": 2.4225847721099854,
      "learning_rate": 3.773159144893112e-05,
      "loss": 0.302,
      "step": 3100
    },
    {
      "epoch": 0.7600950118764845,
      "grad_norm": 1.6271460056304932,
      "learning_rate": 3.7335708630245446e-05,
      "loss": 0.3235,
      "step": 3200
    },
    {
      "epoch": 0.7838479809976246,
      "grad_norm": 4.168035984039307,
      "learning_rate": 3.693982581155978e-05,
      "loss": 0.2724,
      "step": 3300
    },
    {
      "epoch": 0.8076009501187649,
      "grad_norm": 3.4027984142303467,
      "learning_rate": 3.654394299287411e-05,
      "loss": 0.2941,
      "step": 3400
    },
    {
      "epoch": 0.831353919239905,
      "grad_norm": 2.551152229309082,
      "learning_rate": 3.614806017418844e-05,
      "loss": 0.2977,
      "step": 3500
    },
    {
      "epoch": 0.8551068883610451,
      "grad_norm": 2.4584944248199463,
      "learning_rate": 3.5752177355502774e-05,
      "loss": 0.305,
      "step": 3600
    },
    {
      "epoch": 0.8788598574821853,
      "grad_norm": 0.770797610282898,
      "learning_rate": 3.53562945368171e-05,
      "loss": 0.2833,
      "step": 3700
    },
    {
      "epoch": 0.9026128266033254,
      "grad_norm": 3.166243076324463,
      "learning_rate": 3.4960411718131434e-05,
      "loss": 0.278,
      "step": 3800
    },
    {
      "epoch": 0.9263657957244655,
      "grad_norm": 0.9145365953445435,
      "learning_rate": 3.456452889944576e-05,
      "loss": 0.2481,
      "step": 3900
    },
    {
      "epoch": 0.9501187648456056,
      "grad_norm": 1.869080901145935,
      "learning_rate": 3.4168646080760095e-05,
      "loss": 0.2842,
      "step": 4000
    },
    {
      "epoch": 0.9738717339667459,
      "grad_norm": 2.0572636127471924,
      "learning_rate": 3.377276326207443e-05,
      "loss": 0.271,
      "step": 4100
    },
    {
      "epoch": 0.997624703087886,
      "grad_norm": 3.141223192214966,
      "learning_rate": 3.3376880443388755e-05,
      "loss": 0.2847,
      "step": 4200
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8772935779816514,
      "eval_loss": 0.28028979897499084,
      "eval_runtime": 4.4569,
      "eval_samples_per_second": 195.653,
      "eval_steps_per_second": 12.341,
      "step": 4210
    },
    {
      "epoch": 1.0213776722090262,
      "grad_norm": 3.848052740097046,
      "learning_rate": 3.298099762470309e-05,
      "loss": 0.2889,
      "step": 4300
    },
    {
      "epoch": 1.0451306413301662,
      "grad_norm": 3.1890299320220947,
      "learning_rate": 3.258511480601742e-05,
      "loss": 0.3017,
      "step": 4400
    },
    {
      "epoch": 1.0688836104513064,
      "grad_norm": 1.5673003196716309,
      "learning_rate": 3.2189231987331756e-05,
      "loss": 0.2674,
      "step": 4500
    },
    {
      "epoch": 1.0926365795724466,
      "grad_norm": 5.222131252288818,
      "learning_rate": 3.179334916864608e-05,
      "loss": 0.2685,
      "step": 4600
    },
    {
      "epoch": 1.1163895486935866,
      "grad_norm": 2.161618232727051,
      "learning_rate": 3.1397466349960416e-05,
      "loss": 0.2561,
      "step": 4700
    },
    {
      "epoch": 1.1401425178147269,
      "grad_norm": 2.2524287700653076,
      "learning_rate": 3.100158353127474e-05,
      "loss": 0.2796,
      "step": 4800
    },
    {
      "epoch": 1.1638954869358669,
      "grad_norm": 4.230650901794434,
      "learning_rate": 3.060570071258908e-05,
      "loss": 0.2918,
      "step": 4900
    },
    {
      "epoch": 1.187648456057007,
      "grad_norm": 1.9616330862045288,
      "learning_rate": 3.0209817893903407e-05,
      "loss": 0.2795,
      "step": 5000
    },
    {
      "epoch": 1.2114014251781473,
      "grad_norm": 1.7839677333831787,
      "learning_rate": 2.9813935075217737e-05,
      "loss": 0.279,
      "step": 5100
    },
    {
      "epoch": 1.2351543942992875,
      "grad_norm": 0.9742898344993591,
      "learning_rate": 2.941805225653207e-05,
      "loss": 0.2715,
      "step": 5200
    },
    {
      "epoch": 1.2589073634204275,
      "grad_norm": 2.9811806678771973,
      "learning_rate": 2.9022169437846397e-05,
      "loss": 0.2797,
      "step": 5300
    },
    {
      "epoch": 1.2826603325415677,
      "grad_norm": 2.6424477100372314,
      "learning_rate": 2.862628661916073e-05,
      "loss": 0.2751,
      "step": 5400
    },
    {
      "epoch": 1.3064133016627077,
      "grad_norm": 2.8163626194000244,
      "learning_rate": 2.8230403800475058e-05,
      "loss": 0.264,
      "step": 5500
    },
    {
      "epoch": 1.330166270783848,
      "grad_norm": 2.1354784965515137,
      "learning_rate": 2.783452098178939e-05,
      "loss": 0.2715,
      "step": 5600
    },
    {
      "epoch": 1.3539192399049882,
      "grad_norm": 2.9390170574188232,
      "learning_rate": 2.7438638163103725e-05,
      "loss": 0.2602,
      "step": 5700
    },
    {
      "epoch": 1.3776722090261282,
      "grad_norm": 3.358246326446533,
      "learning_rate": 2.7042755344418052e-05,
      "loss": 0.2425,
      "step": 5800
    },
    {
      "epoch": 1.4014251781472684,
      "grad_norm": 2.3765289783477783,
      "learning_rate": 2.6646872525732385e-05,
      "loss": 0.2815,
      "step": 5900
    },
    {
      "epoch": 1.4251781472684084,
      "grad_norm": 2.8924875259399414,
      "learning_rate": 2.6250989707046712e-05,
      "loss": 0.2466,
      "step": 6000
    },
    {
      "epoch": 1.4489311163895486,
      "grad_norm": 1.8983099460601807,
      "learning_rate": 2.5855106888361046e-05,
      "loss": 0.2711,
      "step": 6100
    },
    {
      "epoch": 1.4726840855106889,
      "grad_norm": 3.9189226627349854,
      "learning_rate": 2.5459224069675376e-05,
      "loss": 0.2514,
      "step": 6200
    },
    {
      "epoch": 1.496437054631829,
      "grad_norm": 3.5200252532958984,
      "learning_rate": 2.506334125098971e-05,
      "loss": 0.2613,
      "step": 6300
    },
    {
      "epoch": 1.520190023752969,
      "grad_norm": 8.958297729492188,
      "learning_rate": 2.466745843230404e-05,
      "loss": 0.2707,
      "step": 6400
    },
    {
      "epoch": 1.5439429928741093,
      "grad_norm": 1.2938079833984375,
      "learning_rate": 2.427157561361837e-05,
      "loss": 0.2884,
      "step": 6500
    },
    {
      "epoch": 1.5676959619952493,
      "grad_norm": 3.237495183944702,
      "learning_rate": 2.38756927949327e-05,
      "loss": 0.2465,
      "step": 6600
    },
    {
      "epoch": 1.5914489311163895,
      "grad_norm": 1.4134005308151245,
      "learning_rate": 2.3479809976247034e-05,
      "loss": 0.2532,
      "step": 6700
    },
    {
      "epoch": 1.6152019002375297,
      "grad_norm": 4.224265098571777,
      "learning_rate": 2.3083927157561364e-05,
      "loss": 0.2764,
      "step": 6800
    },
    {
      "epoch": 1.63895486935867,
      "grad_norm": 3.5110981464385986,
      "learning_rate": 2.2688044338875694e-05,
      "loss": 0.2848,
      "step": 6900
    },
    {
      "epoch": 1.66270783847981,
      "grad_norm": 2.0541725158691406,
      "learning_rate": 2.2292161520190024e-05,
      "loss": 0.2435,
      "step": 7000
    },
    {
      "epoch": 1.68646080760095,
      "grad_norm": 2.8434557914733887,
      "learning_rate": 2.1896278701504354e-05,
      "loss": 0.2364,
      "step": 7100
    },
    {
      "epoch": 1.7102137767220902,
      "grad_norm": 3.042985677719116,
      "learning_rate": 2.1500395882818685e-05,
      "loss": 0.2831,
      "step": 7200
    },
    {
      "epoch": 1.7339667458432304,
      "grad_norm": 3.4225876331329346,
      "learning_rate": 2.1104513064133015e-05,
      "loss": 0.2686,
      "step": 7300
    },
    {
      "epoch": 1.7577197149643706,
      "grad_norm": 1.680942177772522,
      "learning_rate": 2.070863024544735e-05,
      "loss": 0.2437,
      "step": 7400
    },
    {
      "epoch": 1.7814726840855108,
      "grad_norm": 1.985785961151123,
      "learning_rate": 2.031274742676168e-05,
      "loss": 0.2588,
      "step": 7500
    },
    {
      "epoch": 1.8052256532066508,
      "grad_norm": 2.2672741413116455,
      "learning_rate": 1.9916864608076012e-05,
      "loss": 0.2599,
      "step": 7600
    },
    {
      "epoch": 1.8289786223277908,
      "grad_norm": 1.3864755630493164,
      "learning_rate": 1.9520981789390342e-05,
      "loss": 0.2706,
      "step": 7700
    },
    {
      "epoch": 1.852731591448931,
      "grad_norm": 3.210967779159546,
      "learning_rate": 1.9125098970704672e-05,
      "loss": 0.2417,
      "step": 7800
    },
    {
      "epoch": 1.8764845605700713,
      "grad_norm": 2.649754285812378,
      "learning_rate": 1.8729216152019003e-05,
      "loss": 0.2571,
      "step": 7900
    },
    {
      "epoch": 1.9002375296912115,
      "grad_norm": 3.21573805809021,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.2713,
      "step": 8000
    },
    {
      "epoch": 1.9239904988123515,
      "grad_norm": 2.378249406814575,
      "learning_rate": 1.7937450514647666e-05,
      "loss": 0.2789,
      "step": 8100
    },
    {
      "epoch": 1.9477434679334917,
      "grad_norm": 2.5564191341400146,
      "learning_rate": 1.7541567695961997e-05,
      "loss": 0.2309,
      "step": 8200
    },
    {
      "epoch": 1.9714964370546317,
      "grad_norm": 2.939850330352783,
      "learning_rate": 1.7145684877276327e-05,
      "loss": 0.2514,
      "step": 8300
    },
    {
      "epoch": 1.995249406175772,
      "grad_norm": 1.6547355651855469,
      "learning_rate": 1.6749802058590657e-05,
      "loss": 0.26,
      "step": 8400
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8818807339449541,
      "eval_loss": 0.28311797976493835,
      "eval_runtime": 4.1821,
      "eval_samples_per_second": 208.509,
      "eval_steps_per_second": 13.151,
      "step": 8420
    },
    {
      "epoch": 2.019002375296912,
      "grad_norm": 2.1645288467407227,
      "learning_rate": 1.6353919239904987e-05,
      "loss": 0.2502,
      "step": 8500
    },
    {
      "epoch": 2.0427553444180524,
      "grad_norm": 3.4676668643951416,
      "learning_rate": 1.5958036421219317e-05,
      "loss": 0.2525,
      "step": 8600
    },
    {
      "epoch": 2.0665083135391926,
      "grad_norm": 2.6339612007141113,
      "learning_rate": 1.556215360253365e-05,
      "loss": 0.2333,
      "step": 8700
    },
    {
      "epoch": 2.0902612826603324,
      "grad_norm": 1.490004062652588,
      "learning_rate": 1.5166270783847983e-05,
      "loss": 0.2744,
      "step": 8800
    },
    {
      "epoch": 2.1140142517814726,
      "grad_norm": 2.5503439903259277,
      "learning_rate": 1.4770387965162313e-05,
      "loss": 0.25,
      "step": 8900
    },
    {
      "epoch": 2.137767220902613,
      "grad_norm": 1.6679606437683105,
      "learning_rate": 1.4374505146476643e-05,
      "loss": 0.2529,
      "step": 9000
    },
    {
      "epoch": 2.161520190023753,
      "grad_norm": 2.895287036895752,
      "learning_rate": 1.3978622327790975e-05,
      "loss": 0.2452,
      "step": 9100
    },
    {
      "epoch": 2.1852731591448933,
      "grad_norm": 4.596199989318848,
      "learning_rate": 1.3582739509105305e-05,
      "loss": 0.2602,
      "step": 9200
    },
    {
      "epoch": 2.209026128266033,
      "grad_norm": 2.9071359634399414,
      "learning_rate": 1.3186856690419635e-05,
      "loss": 0.2649,
      "step": 9300
    },
    {
      "epoch": 2.2327790973871733,
      "grad_norm": 1.5223006010055542,
      "learning_rate": 1.2790973871733966e-05,
      "loss": 0.227,
      "step": 9400
    },
    {
      "epoch": 2.2565320665083135,
      "grad_norm": 2.8651320934295654,
      "learning_rate": 1.2395091053048298e-05,
      "loss": 0.2395,
      "step": 9500
    },
    {
      "epoch": 2.2802850356294537,
      "grad_norm": 2.480520248413086,
      "learning_rate": 1.1999208234362628e-05,
      "loss": 0.2498,
      "step": 9600
    },
    {
      "epoch": 2.304038004750594,
      "grad_norm": 0.773613452911377,
      "learning_rate": 1.1603325415676961e-05,
      "loss": 0.2434,
      "step": 9700
    },
    {
      "epoch": 2.3277909738717337,
      "grad_norm": 4.113451957702637,
      "learning_rate": 1.1207442596991292e-05,
      "loss": 0.2599,
      "step": 9800
    },
    {
      "epoch": 2.351543942992874,
      "grad_norm": 2.0072646141052246,
      "learning_rate": 1.0811559778305622e-05,
      "loss": 0.2576,
      "step": 9900
    },
    {
      "epoch": 2.375296912114014,
      "grad_norm": 1.199323058128357,
      "learning_rate": 1.0415676959619954e-05,
      "loss": 0.2635,
      "step": 10000
    },
    {
      "epoch": 2.3990498812351544,
      "grad_norm": 0.5333179831504822,
      "learning_rate": 1.0019794140934284e-05,
      "loss": 0.272,
      "step": 10100
    },
    {
      "epoch": 2.4228028503562946,
      "grad_norm": 2.691671848297119,
      "learning_rate": 9.623911322248614e-06,
      "loss": 0.2532,
      "step": 10200
    },
    {
      "epoch": 2.446555819477435,
      "grad_norm": 2.6933906078338623,
      "learning_rate": 9.228028503562946e-06,
      "loss": 0.2629,
      "step": 10300
    },
    {
      "epoch": 2.470308788598575,
      "grad_norm": 5.019940376281738,
      "learning_rate": 8.832145684877278e-06,
      "loss": 0.2487,
      "step": 10400
    },
    {
      "epoch": 2.494061757719715,
      "grad_norm": 2.9535059928894043,
      "learning_rate": 8.436262866191608e-06,
      "loss": 0.2423,
      "step": 10500
    },
    {
      "epoch": 2.517814726840855,
      "grad_norm": 0.9817441701889038,
      "learning_rate": 8.040380047505938e-06,
      "loss": 0.2498,
      "step": 10600
    },
    {
      "epoch": 2.5415676959619953,
      "grad_norm": 2.1299362182617188,
      "learning_rate": 7.64449722882027e-06,
      "loss": 0.2475,
      "step": 10700
    },
    {
      "epoch": 2.5653206650831355,
      "grad_norm": 3.166530132293701,
      "learning_rate": 7.248614410134601e-06,
      "loss": 0.2508,
      "step": 10800
    },
    {
      "epoch": 2.5890736342042757,
      "grad_norm": 1.8259873390197754,
      "learning_rate": 6.852731591448931e-06,
      "loss": 0.2583,
      "step": 10900
    },
    {
      "epoch": 2.6128266033254155,
      "grad_norm": 4.335933685302734,
      "learning_rate": 6.4568487727632614e-06,
      "loss": 0.2319,
      "step": 11000
    },
    {
      "epoch": 2.6365795724465557,
      "grad_norm": 3.1998541355133057,
      "learning_rate": 6.060965954077593e-06,
      "loss": 0.2437,
      "step": 11100
    },
    {
      "epoch": 2.660332541567696,
      "grad_norm": 4.035822868347168,
      "learning_rate": 5.665083135391924e-06,
      "loss": 0.2485,
      "step": 11200
    },
    {
      "epoch": 2.684085510688836,
      "grad_norm": 3.411653757095337,
      "learning_rate": 5.269200316706255e-06,
      "loss": 0.2404,
      "step": 11300
    },
    {
      "epoch": 2.7078384798099764,
      "grad_norm": 3.3246548175811768,
      "learning_rate": 4.8733174980205864e-06,
      "loss": 0.2654,
      "step": 11400
    },
    {
      "epoch": 2.731591448931116,
      "grad_norm": 2.3672738075256348,
      "learning_rate": 4.4774346793349175e-06,
      "loss": 0.2064,
      "step": 11500
    },
    {
      "epoch": 2.7553444180522564,
      "grad_norm": 2.485325336456299,
      "learning_rate": 4.081551860649248e-06,
      "loss": 0.2743,
      "step": 11600
    },
    {
      "epoch": 2.7790973871733966,
      "grad_norm": 2.3418068885803223,
      "learning_rate": 3.685669041963579e-06,
      "loss": 0.2684,
      "step": 11700
    },
    {
      "epoch": 2.802850356294537,
      "grad_norm": 2.290051221847534,
      "learning_rate": 3.2897862232779097e-06,
      "loss": 0.2506,
      "step": 11800
    },
    {
      "epoch": 2.826603325415677,
      "grad_norm": 1.3345896005630493,
      "learning_rate": 2.8939034045922408e-06,
      "loss": 0.2481,
      "step": 11900
    },
    {
      "epoch": 2.850356294536817,
      "grad_norm": 1.4090330600738525,
      "learning_rate": 2.498020585906572e-06,
      "loss": 0.2393,
      "step": 12000
    },
    {
      "epoch": 2.8741092636579575,
      "grad_norm": 2.6525959968566895,
      "learning_rate": 2.1021377672209024e-06,
      "loss": 0.246,
      "step": 12100
    },
    {
      "epoch": 2.8978622327790973,
      "grad_norm": 1.930294156074524,
      "learning_rate": 1.7062549485352335e-06,
      "loss": 0.2539,
      "step": 12200
    },
    {
      "epoch": 2.9216152019002375,
      "grad_norm": 3.1263346672058105,
      "learning_rate": 1.3103721298495645e-06,
      "loss": 0.2331,
      "step": 12300
    },
    {
      "epoch": 2.9453681710213777,
      "grad_norm": 4.0380706787109375,
      "learning_rate": 9.144893111638954e-07,
      "loss": 0.2739,
      "step": 12400
    },
    {
      "epoch": 2.969121140142518,
      "grad_norm": 2.5808563232421875,
      "learning_rate": 5.186064924782265e-07,
      "loss": 0.2297,
      "step": 12500
    },
    {
      "epoch": 2.992874109263658,
      "grad_norm": 5.96212911605835,
      "learning_rate": 1.227236737925574e-07,
      "loss": 0.2478,
      "step": 12600
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8864678899082569,
      "eval_loss": 0.28322896361351013,
      "eval_runtime": 4.0353,
      "eval_samples_per_second": 216.091,
      "eval_steps_per_second": 13.63,
      "step": 12630
    }
  ],
  "logging_steps": 100,
  "max_steps": 12630,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6805923233854464.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
